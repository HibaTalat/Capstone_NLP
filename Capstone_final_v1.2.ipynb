{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE MODEL ANSWER!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#warnings :)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tkinter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Mongo DB Setup \n",
    "\n",
    "Pulling data from MongoAtlas \n",
    "https://cloud.mongodb.com/v2/6121b9107a12b36bfc551739#metrics/replicaSet/612313cda2e2d25be365b868/explorer/sentences/random_sentence/find\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/Users/hibatalat/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: pymongo in ./opt/anaconda3/lib/python3.9/site-packages (4.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/Users/hibatalat/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: textblob in ./opt/anaconda3/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in ./opt/anaconda3/lib/python3.9/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in ./opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in ./opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: tqdm in ./opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "The number of sentences in the database:15\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#MongoDB\n",
    "from pymongo import MongoClient\n",
    "import dns\n",
    "\n",
    "client = MongoClient(\"mongodb+srv://user:test@cluster0.3o3d3.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "\n",
    "db = client.get_database('sentences')\n",
    "records=db['random_sentence']\n",
    "\n",
    "#count document without providing any filter records.count_documents({})\n",
    "total= db.random_sentence.count_documents({})\n",
    "print(\"==================\")\n",
    "print(f'The number of sentences in the database:{total}')\n",
    "print(\"==================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "{'sentence': 'Lets all be unique together until we realise we are all the same.'}\n",
      "==================\n",
      "{'' 'Lets all be unique together until we realise we are all the same.'}\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#Randomly picked sentence\n",
    "import random\n",
    "cur=db.random_sentence.find({},{\"_id\":0})\n",
    "x= list(cur)\n",
    "rand_sentence= random.choice(x)\n",
    "print(\"==================\")\n",
    "print(rand_sentence)\n",
    "print(\"==================\")\n",
    "\n",
    "text = str(rand_sentence)\n",
    "word = 'sentence'\n",
    "text = text.replace(word, \"\")\n",
    "word1 =':'\n",
    "text1 = text.replace(word1,\"\")\n",
    "#printing a random sentence from the 15 sentences we have in Mongo DB\n",
    "#The word sentence and colon is removed from the sentence and further will clean the sentence below\n",
    "print(text1)\n",
    "print(\"==================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverted to lower text: {'': 'lets all be unique together until we realise we are all the same.'}\n",
      "==================\n",
      "Punctuation removed :  lets all be unique together until we realise we are all the same\n",
      "==================\n",
      "Whitespace is removed :lets all be unique together until we realise we are all the same\n",
      "==================\n",
      "Numbers are removed :lets all be unique together until we realise we are all the same\n",
      "==================\n",
      "Named entities are listed as : (S\n",
      "  lets/NNS\n",
      "  all/DT\n",
      "  be/VB\n",
      "  unique/JJ\n",
      "  together/RB\n",
      "  until/IN\n",
      "  we/PRP\n",
      "  realise/VBP\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  all/PDT\n",
      "  the/DT\n",
      "  same/JJ)\n",
      "==================\n",
      "Stop words are removed : ['lets', 'unique', 'together', 'realise']\n",
      "==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the output is :  let unique together recognize\n",
      "==================\n",
      "The lematized version of the sentence is :  let unique together recognize\n",
      "==================\n",
      "The stemmized version of the sentence is :  let unique together recognize\n",
      "The complete sentence including synonym , lemmatized and stemmized is : lets all be unique together until we realise we are all the same. let unique together recognize. let unique together recognize. let unique together recognize\n",
      "==================\n",
      "=====================\n",
      "\n",
      "The output for word tokenization : ['lets', 'all', 'be', 'unique', 'together', 'until', 'we', 'realise', 'we', 'are', 'all', 'the', 'same', '.', 'let', 'unique', 'together', 'recognize', '.', 'let', 'unique', 'together', 'recognize', '.', 'let', 'unique', 'together', 'recognize']\n",
      "==================\n",
      "The spell check has been performed on the sentence:lets all be unique together until we realise we are all the same. let unique together recognize. let unique together recognize. let unique together recognize\n",
      "==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "#lower the mongo sentence\n",
    "text_lower= text.lower()\n",
    "print(f\"Coverted to lower text: {text_lower}\")\n",
    "print(\"==================\")\n",
    "\n",
    "#punctuation removed\n",
    "text_punc = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(f\"Punctuation removed : {text_punc }\")\n",
    "print(\"==================\")\n",
    "\n",
    "\n",
    "#Removing whitespace\n",
    "\n",
    "no_ws_text = text_punc.strip()\n",
    "\n",
    "print(f\"Whitespace is removed :{no_ws_text}\")\n",
    "print(\"==================\")\n",
    "\n",
    "#Remove Numbers\n",
    "import re\n",
    "no_num_text = re.sub(r'\\d+','',no_ws_text)\n",
    "print(f\"Numbers are removed :{no_num_text}\")\n",
    "print(\"==================\")\n",
    "\n",
    "\n",
    "#Named Entity Recognition\n",
    "\n",
    "from nltk import word_tokenize,pos_tag,ne_chunk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "name_text= ne_chunk(pos_tag(word_tokenize(no_num_text)))\n",
    "print(f\"Named entities are listed as : {name_text}\")\n",
    "print(\"==================\")\n",
    "\n",
    "#Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "postpa = [word for word in no_num_text.split()if word not in stoplist]\n",
    "print(f\"Stop words are removed : {postpa}\")\n",
    "print(\"==================\")\n",
    "\n",
    "#Replacing with synonyms\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from random import randint\n",
    "import nltk.data\n",
    "\n",
    "# # Load a text file if required\n",
    "# text = \"Pete ate a large cake. Sam has a big mouth.\"\n",
    "output = \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Get the list of words from the entire text\n",
    "words = postpa\n",
    "\n",
    "# Identify the parts of speech\n",
    "tagged = nltk.pos_tag(words)\n",
    "\n",
    "for i in range(0,len(words)):\n",
    "    replacements = []\n",
    "\n",
    "    # Only replace nouns with nouns, vowels with vowels etc.\n",
    "    for syn in wordnet.synsets(words[i]):\n",
    "\n",
    "        # Do not attempt to replace proper nouns or determiners\n",
    "        if tagged[i][1] == 'NNP' or tagged[i][1] == 'DT':\n",
    "            break\n",
    "        \n",
    "        # The tokenizer returns strings like NNP, VBP etc\n",
    "        # but the wordnet synonyms has tags like .n.\n",
    "        # So we extract the first character from NNP ie n\n",
    "        # then we check if the dictionary word has a .n. or not \n",
    "        word_type = tagged[i][1][0].lower()\n",
    "        if syn.name().find(\".\"+word_type+\".\"):\n",
    "            # extract the word only\n",
    "            r = syn.name()[0:syn.name().find(\".\")]\n",
    "            replacements.append(r)\n",
    "\n",
    "    if len(replacements) > 0:\n",
    "        # Choose a random replacement\n",
    "        replacement = replacements[randint(0,len(replacements)-1)]\n",
    "        output = output + \" \" + replacement\n",
    "    else:\n",
    "        # If no replacement could be found, then just use the\n",
    "        # original word\n",
    "        output = output + \" \" + words[i]\n",
    "\n",
    "print(f'the output is : {output}')\n",
    "print(\"==================\")\n",
    "\n",
    "#Stamming and Lemminization \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "porter = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_string = ''.join([lemma.lemmatize(words) for words in output])\n",
    "print(f'The lematized version of the sentence is : {lemmatized_string}')  \n",
    "print(\"==================\")\n",
    "\n",
    "# print(lemma.lemmatize(postpa))\n",
    "stem_string = ''.join([porter.stem(words)for words in output])\n",
    "print(f'The stemmized version of the sentence is : {stem_string}')\n",
    "\n",
    "complete_text= no_num_text+\".\"+output+ \".\"+ stem_string+\".\"+lemmatized_string\n",
    "print(f'The complete sentence including synonym , lemmatized and stemmized is : {complete_text}')\n",
    "print(\"==================\")\n",
    "\n",
    "# from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "# TreebankWordDetokenizer().detokenize([stem_string])\n",
    "# TreebankWordDetokenizer().detokenize([lemmatized_string])\n",
    "print(\"=====================\\n\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs_text = word_tokenize(complete_text)\n",
    "print(f\"The output for word tokenization : {tokenized_docs_text}\")\n",
    "print(\"==================\")\n",
    "\n",
    "#Spellcheck using textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "spell_check_text= TextBlob(complete_text)\n",
    "\n",
    "print(f'The spell check has been performed on the sentence:{spell_check_text.correct()}')\n",
    "print(\"==================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating GUI for user to add a sentence\n",
    "Using Tkinter python library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      " You have provided this sentence:\n",
      "===============\n",
      "Converted the sentence to lower case:\n",
      "the output after removing punctuations: \n",
      "===============\n",
      "Remove the whitespaces in between sentence:\n",
      "===============\n",
      "===============\n",
      "Removing the numbers:\n",
      "===============\n",
      "Named entities are listed as : (S )\n",
      "==================\n",
      "The output for word tokenization : []\n",
      "===============\n",
      "===============\n",
      "The output after removing stopwords:[]\n",
      "===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Top level window\n",
    "\n",
    "import tkinter as tk\n",
    "frame = tk.Tk()\n",
    "frame.title(\"TextBox Input\")\n",
    "frame.geometry('1000x1000')\n",
    "\n",
    "# Function for getting Input from textbox and printing it at label widget\n",
    " \n",
    "def printInput():\n",
    "    global inp\n",
    "    inp = inputtxt.get(1.0, \"end-1c\")\n",
    "    #print(f\"Your current input is :{inp}\")\n",
    "    lbl.config(text = \"Provided Input: \"+inp)\n",
    "\n",
    "#Textbox creation\n",
    "inputtxt = tk.Text(frame,height = 20, width = 40,bg = \"light blue\")\n",
    "\n",
    "inputtxt.pack()\n",
    "\n",
    "#Button Creation\n",
    "printButton = tk.Button(frame,text = \"Enter\",command = printInput)\n",
    "printButton.pack()\n",
    "#print(inp)\n",
    "\n",
    "\n",
    "#Convert to lower case the user input\n",
    "print(\"===============\")\n",
    "print(f' You have provided this sentence:{inp}')\n",
    "print(\"===============\")\n",
    "\n",
    "import string\n",
    "raw_docs = inp.lower()\n",
    "print(f'Converted the sentence to lower case:{raw_docs}')\n",
    "\n",
    "#Remove punctuation\n",
    "\n",
    "my_string = raw_docs\n",
    "output_withno_punc = my_string.translate(str.maketrans('', '', string.punctuation))\n",
    "print(f\"the output after removing punctuations: {output_withno_punc}\")\n",
    "print(\"===============\")\n",
    "\n",
    "#Removing whitespace\n",
    "\n",
    "no_ws_doc = output_withno_punc.strip()\n",
    "print(f'Remove the whitespaces in between sentence:{no_ws_doc}')\n",
    "print(\"===============\")\n",
    "\n",
    "#Remove Numbers\n",
    "import re\n",
    "no_num = re.sub(r'\\d+','',no_ws_doc)\n",
    "print(\"===============\")\n",
    "\n",
    "print(f'Removing the numbers:{no_num}')\n",
    "print(\"===============\")\n",
    "#Named Entities\n",
    "from nltk import word_tokenize,pos_tag,ne_chunk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "name_text1= ne_chunk(pos_tag(word_tokenize(no_num)))\n",
    "print(f\"Named entities are listed as : {name_text1}\")\n",
    "print(\"==================\")\n",
    "\n",
    "#Word tokenize\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = word_tokenize(no_ws_doc)\n",
    "print(f\"The output for word tokenization : {tokenized_docs}\")\n",
    "print(\"===============\")\n",
    "\n",
    "# #Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "post_doc = [word for word in no_num.split()if not word in stopwords.words()]\n",
    "print(\"===============\")\n",
    "print (f\"The output after removing stopwords:{post_doc}\")\n",
    "\n",
    "print(\"===============\")\n",
    "\n",
    "\n",
    "lbl = tk.Label(frame, text = \"Enter the sentence to compare\", font='bold')\n",
    "lbl.pack()\n",
    "lab_msg = tk.Label(frame , text = \"The Sentence from the Database is below:\", font='bold')\n",
    "lab_msg.pack()\n",
    "lbl1 = tk.Label(frame, text =no_ws_text )\n",
    "lbl1.pack()\n",
    "\n",
    "frame.mainloop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "The output after replacing with the synonyms is : \n",
      "==================\n",
      "The lemmatized version of the sentence is : \n",
      "==================\n",
      "The stemmized version of the sentence is : \n",
      "==================\n",
      "The complete sentence including synonym , lemmatized and stemmized is : i have a%cute cat...\n",
      "==================\n",
      "Spell check:i have a%cut cat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hibatalat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Synonyms , Lemmatize and Stemming \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from random import randint\n",
    "import nltk.data\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "porter = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "output_in = \"\"\n",
    "\n",
    "# Get the list of words from the entire text\n",
    "words = post_doc\n",
    "\n",
    "# Identify the parts of speech\n",
    "tagged = nltk.pos_tag(words)\n",
    "\n",
    "for i in range(0,len(words)):\n",
    "    replacements = []\n",
    "\n",
    "    # Only replace nouns with nouns, vowels with vowels etc.\n",
    "    for syn in wordnet.synsets(words[i]):\n",
    "\n",
    "        # Do not attempt to replace proper nouns or determiners\n",
    "        if tagged[i][1] == 'NNP' or tagged[i][1] == 'DT':\n",
    "            break\n",
    "        \n",
    "        # The tokenizer returns strings like NNP, VBP etc\n",
    "        # but the wordnet synonyms has tags like .n.\n",
    "        # So we extract the first character from NNP ie n\n",
    "        # then we check if the dictionary word has a .n. or not \n",
    "        word_type = tagged[i][1][0].lower()\n",
    "        if syn.name().find(\".\"+word_type+\".\"):\n",
    "            # extract the word only\n",
    "            r = syn.name()[0:syn.name().find(\".\")]\n",
    "            replacements.append(r)\n",
    "\n",
    "    if len(replacements) > 0:\n",
    "        # Choose a random replacement\n",
    "        replacement = replacements[randint(0,len(replacements)-1)]\n",
    "        output_in = output_in + \" \" + replacement\n",
    "    else:\n",
    "        # If no replacement could be found, then just use the\n",
    "        # original word\n",
    "        output_in = output_in + \" \" + words[i]\n",
    "print(\"==================\")\n",
    "\n",
    "print(f'The output after replacing with the synonyms is : {output_in}')\n",
    "\n",
    "#Stamming and Lemminization \n",
    "\n",
    "lemmatized_string1 = ''.join([lemma.lemmatize(words) for words in output_in])\n",
    "print(\"==================\")\n",
    "\n",
    "print(f'The lemmatized version of the sentence is : {lemmatized_string1}')  \n",
    "\n",
    "# # print(lemma.lemmatize(postpa))\n",
    "stem_string1 = ''.join([porter.stem(words)for words in output_in])\n",
    "print(\"==================\")\n",
    "\n",
    "print(f'The stemmized version of the sentence is : {stem_string1}')\n",
    "\n",
    "\n",
    "complete_text1= inp +\".\"+ output_in+ \".\"+ stem_string1+\".\"+lemmatized_string1\n",
    "print(\"==================\")\n",
    "\n",
    "print(f'The complete sentence including synonym , lemmatized and stemmized is : {complete_text1}')\n",
    "\n",
    "\n",
    "#Spellcheck using textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "spell_check_text1= TextBlob(complete_text1)\n",
    "print(\"==================\")\n",
    "print(f\"Spell check:{spell_check_text1.correct()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing two sentences and returning the percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of your text match is :0.13559322033898305\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "similarity = SequenceMatcher(None, spell_check_text, spell_check_text1).ratio()\n",
    "# print(similarity)\n",
    "\n",
    "print(f'The ratio of your text match is :{similarity}')\n",
    "similarity1= similarity *100\n",
    "\n",
    "if similarity1 >= 70:\n",
    "    result =\"PASS\"\n",
    "else:\n",
    "    result=\"FAIL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "frame = tk.Tk()\n",
    "frame.title(\"TextBox Input\")\n",
    "frame.geometry('1000x1000')\n",
    "lbl2 = tk.Label(frame, text =result)\n",
    "lbl2.pack()\n",
    "frame.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
